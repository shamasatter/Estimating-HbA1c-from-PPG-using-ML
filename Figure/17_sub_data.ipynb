{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install heartpy\n",
    "# %pip install tsfel\n",
    "# %pip install librosa\n",
    "# %pip install statsmodels\n",
    "# %pip install lightgbm\n",
    "# %pip install catboost\n",
    "# %pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heartpy as hp\n",
    "from   matplotlib import pyplot as plt\n",
    "import tsfel\n",
    "import scipy \n",
    "import sklearn as sk\n",
    "from   sklearn.ensemble import RandomForestRegressor\n",
    "import librosa\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import statsmodels.regression\n",
    "from   warnings import filterwarnings\n",
    "import lightgbm as ltb\n",
    "from   sklearn import metrics\n",
    "import catboost as cab\n",
    "from   sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from   datetime import datetime\n",
    "from   sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from   sklearn.metrics import r2_score\n",
    "from   IPython.display import clear_output\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPG_dataDirectory = 'E:\\OneDrive - KookminUNIV\\KMU==Lab\\MCSP==Research-Papers\\SCI-02\\Software\\ProgrammingCodes\\DataBase\\\\13_raw_signal\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPG_data =  [   \n",
    "            'turja.csv',   \n",
    "            'THK.csv',   \n",
    "            'KDK.csv',             \n",
    "            'shama.csv',             \n",
    "            'JHO.csv', \n",
    "            'JHC.csv',           \n",
    "            'KHJ.csv',                       \n",
    "            'JKY.csv',          \n",
    "            'YSB.csv', \n",
    "            'SJL.csv',\n",
    "            'SSL.csv',  \n",
    "            'SJL.csv',\n",
    "            'YJL.csv',\n",
    "            'Toma.csv',\n",
    "            'YIP.csv',\n",
    "            'YHP.csv',\n",
    "            'JTS.csv'          \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = 'E:\\OneDrive - KookminUNIV\\KMU==Lab\\MCSP==Research-Papers\\SCI-02\\Software\\ProgrammingCodes\\DataBase\\\\13_raw_signal\\\\turja.csv'\n",
    "filePath \n",
    "\n",
    "#THis is how code works. You have to check each output. When ther is any numerical value at first. System can't identify right info. That is not the right path.\n",
    "#  You didn't even add '\\' in your code\n",
    "# TO solve this I used '\\\\' as it is getting 1 in next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat    = pd.read_csv(filePath, delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PPG Data\n",
    "# define parent directory of PPG files\n",
    "# PPG_dataDirectory = 'gdrive/My Drive/Colab Notebooks/SCI#02/DataBase/data/'\n",
    "PPG_dataDirectory = 'E:\\OneDrive - KookminUNIV\\KMU==Lab\\MCSP==Research-Papers\\SCI-02\\Software\\ProgrammingCodes\\DataBase\\\\13_raw_signal\\\\'\n",
    "# PPG CSV filenames\n",
    "PPG_data =  [   \n",
    "            'turja.csv',   \n",
    "            'THK.csv',   \n",
    "            'KDK.csv',             \n",
    "            'shama.csv',             \n",
    "            'JHO.csv', \n",
    "            'JHC.csv' ,          \n",
    "            'KHJ.csv',                       \n",
    "            'JKY.csv',          \n",
    "            'YSB.csv', \n",
    "            'SJL.csv',\n",
    "            'SSL.csv',  \n",
    "            'SJL.csv',\n",
    "            'YJL.csv',\n",
    "            'Toma.csv',\n",
    "            'YIP.csv',\n",
    "            'YHP.csv',\n",
    "            'JTS.csv'                       \n",
    "            ]\n",
    "# Assigning complete filename of PPG CSV data\n",
    "PPG_csvData = []\n",
    "for fileName in PPG_data:\n",
    "    PPG_csvData.append(PPG_dataDirectory+fileName) # The fileName was a variable in previous code\n",
    "\n",
    "\n",
    "## HbA1c data\n",
    "HbA1c_data  =   [ \n",
    "                5.2,5.5,5.7,5.6,6.2,5.8,7,7.4,7.6,6,7.4,6,5.8,5.9,6,5.5,7.7\n",
    "                ]\n",
    "\n",
    "\n",
    "## SpO2 data\n",
    "SpO2_data   =   [ 99,96,98,98,97,97,97,96,96,97,98,97,98,98,98,98,98          \n",
    "                ]\n",
    "\n",
    "\n",
    "## Age data\n",
    "Age_data    =   [ \n",
    "                25,34,65,27,55,58,55,56,54,29,28,30,27,58,64\n",
    "            \n",
    "                ]\n",
    "\n",
    "\n",
    "## Gender data\n",
    "Gender_data =   [ \n",
    "                'M','M','M','F','M','M','M','M','M','M',\n",
    "                'M','M','F','M','F'\n",
    "                ]\n",
    "\n",
    "\n",
    "                \n",
    "## BMI data\n",
    "BMI_data    =   [ \n",
    "                26.9,26.4,22.2,27,25.4,29,21.7,26,28.71,27.16,28.73,27.16,22.09,26.2,21.07,19.53,26.03\n",
    "                ]\n",
    "\n",
    "\n",
    "print('*****************PPG_csvData******************')\n",
    "print(PPG_csvData)\n",
    "print('Data counted = '+ str(len(PPG_csvData)))\n",
    "print('******************HbA1c_data******************')\n",
    "print(HbA1c_data)\n",
    "print('Data counted = '+ str(len(HbA1c_data)))\n",
    "print('******************SpO2_data*******************')\n",
    "print(SpO2_data)\n",
    "print('Data counted = '+ str(len(SpO2_data)))\n",
    "print('*******************Age_data*******************')\n",
    "print(Age_data)\n",
    "print('Data counted = '+ str(len(Age_data)))\n",
    "print('*****************Gender_data******************')\n",
    "print(Gender_data)\n",
    "print('Data counted = '+ str(len(Gender_data)))\n",
    "\n",
    "\n",
    "print('******************BMI_data********************')\n",
    "print(BMI_data)\n",
    "print('Data counted = '+ str(len(BMI_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPG_csvArray  = np.asarray(PPG_csvData)\n",
    "HbA1c_Array   = np.asarray(HbA1c_data)\n",
    "SpO2_Array    = np.asarray(SpO2_data)\n",
    "Age_Array     = np.asarray(Age_data)\n",
    "Gender_Array  = np.asarray(Gender_data)\n",
    "BMI_Array     = np.asarray(BMI_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_PPG(PPG, drawFigure=True):\n",
    "\n",
    "    for i in range (0,len(PPG)):\n",
    "        \n",
    "        # Getting corresponding Combined PPG Data \n",
    "        dat    = pd.read_csv(PPG[i], delimiter=',', header=None)\n",
    "        print('********************************')\n",
    "        print('Index='+str(i)+' || '+str(PPG[i]))\n",
    "        print('********************************')\n",
    "        bs = dat.iloc[:,2]\n",
    "        gs = dat.iloc[:,1]\n",
    "        rs = dat.iloc[:,0]\n",
    "   \n",
    "    \n",
    "        # Visualization **Don't mention inside the pseudo code**\n",
    "        if drawFigure:\n",
    "           plt.rcParams.update(plt.rcParamsDefault)\n",
    "           plt.rcParams['font.size'] = '8'\n",
    "           fig,axs = plt.subplots(3, figsize=(20, 5))\n",
    "           axs[0].set(title='Visualization of Wrist PPG Signal')\n",
    "           axs[0].plot(bs, color='blue', label='blue signal')\n",
    "           axs[0].set(ylabel = ' ')\n",
    "           axs[0].legend(loc=1)\n",
    "           axs[0].grid(True)\n",
    "           axs[1].plot(gs, color='green', label='green signal')\n",
    "           axs[1].set(ylabel = ' ', xlabel = ' ')\n",
    "           axs[1].legend(loc=1)\n",
    "           axs[1].grid(True)\n",
    "           axs[2].plot(rs, color='red', label='red signal')\n",
    "           axs[2].set(ylabel = ' ', xlabel = ' ')\n",
    "           axs[2].legend(loc=1)\n",
    "           axs[2].grid(True)\n",
    "           plt.show()\n",
    "        \n",
    "    # Return\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bland_altman_plot(data1, data2):\n",
    "    data1     = np.asarray(data1)\n",
    "    data2     = np.asarray(data2)\n",
    "    mean      = np.mean([data1, data2], axis=0)\n",
    "    diff      = data1 - data2                   # Difference between data1 and data2\n",
    "    md        = np.mean(diff)                   # Mean of the difference\n",
    "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
    "\n",
    "    plt.scatter(mean, diff,   c = 'black')\n",
    "    plt.axhline(md,           color='green', linestyle='--')\n",
    "    plt.axhline(md + 1.96*sd, color='red',   linestyle='--')\n",
    "    plt.axhline(md - 1.96*sd, color='red',   linestyle='--')\n",
    "    plt.xlabel(\"Average of the two measures\", fontsize = 14)\n",
    "    plt.ylabel(\"Difference between the two measures\", fontsize = 14)\n",
    "    plt.title(\"Bland-Altman plot for estimated %HbA1c values\", fontsize = 14)\n",
    "    # plt.savefig('./Figure/RF_trans_BA_Plot.png', bbox_inches = \"tight\", transparent = True, dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle, Wedge, Polygon, Path\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "def error_grid_analysis(ref,est):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    polygons = []\n",
    "\n",
    "    A  = [[4.0,4.0],[5.7,4.0],[5.7,5.4],[7.2,6.5],[12.0,6.5],[12.0,12.0],[6.5,12.0],[6.5,7.2],[5.4,5.7],[4.0,5.7]]\n",
    "    Bd = [[5.7,4.0],[5.7,5.4],[7.2,6.5],[12.0,6.5],[12.0,5.7],[6.5,5.7],[6.5,4.0]]\n",
    "    Bu = [[4.0,5.7],[5.4,5.7],[6.5,7.2],[6.5,12.0],[5.7,12.0],[5.7,6.5],[4.0,6.5]]\n",
    "    Cd = [[6.5,4.0],[6.5,5.7],[12.0,5.7],[12.0,4.0]]\n",
    "    Cu = [[4.0,6.5],[5.7,6.5],[5.7,12.0],[4.0,12.0]]\n",
    "\n",
    "    # print(np.asarray(A).shape)\n",
    "    pA  = Polygon(np.asarray(A))\n",
    "    pBd = Polygon(np.asarray(Bd))\n",
    "    pBu = Polygon(np.asarray(Bu))\n",
    "    pCd = Polygon(np.asarray(Cd))\n",
    "    pCu = Polygon(np.asarray(Cu))\n",
    "\n",
    "    polygons.append(pA)\n",
    "    polygons.append(pBd)\n",
    "    polygons.append(pBu)\n",
    "    polygons.append(pCd)\n",
    "    polygons.append(pCu)\n",
    "\n",
    "\n",
    "    p = PatchCollection(polygons)\n",
    "    p.set_color([[0,1,0,0.3],[1,1,0,0.3],[1,1,0,0.3],[1,0,0,0.3],[1,0,0,0.3]])\n",
    "    ax.add_collection(p)\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot([4.0,12.0],[4.0,12.0])\n",
    "\n",
    "\n",
    "    x = ref\n",
    "    y = est\n",
    "\n",
    "    sA = []\n",
    "    sB = []\n",
    "    sC = []\n",
    "    for i in range(len(y)):\n",
    "        sA.append(int(pA.contains_point ([x[i],y[i]]))*30)\n",
    "        sB.append(int(pBd.contains_point([x[i],y[i]]) or pBu.contains_point([x[i],y[i]]))*30)\n",
    "        sC.append(int(pCd.contains_point([x[i],y[i]]) or pCu.contains_point([x[i],y[i]]))*30)\n",
    "\n",
    "\n",
    "    plt.scatter(x,y,s=sA,color=[0.227, 0.513, 0.282,1])\n",
    "    plt.scatter(x,y,s=sB,color=[0.886, 0.654, 0.133,1])\n",
    "    plt.scatter(x,y,s=sC,color=[0.941, 0, 0.098,1])\n",
    "\n",
    "    lx = np.linspace(min(x), max(x), 100)\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(lx,p(lx))\n",
    "\n",
    "\n",
    "    plt.text(10.5,9,'A',fontsize=26)\n",
    "    plt.text(9,10.5,'A',fontsize=26)\n",
    "    plt.text(10.5,5.94,'B',fontsize=26)\n",
    "    plt.text(5.94,10.5,'B',fontsize=26)\n",
    "    plt.text(10.5,4.7,'C',fontsize=26)\n",
    "    plt.text(4.7,10.5,'C',fontsize=26)\n",
    "    fig.set_figheight(8)\n",
    "    fig.set_figwidth(8)\n",
    "    plt.xlim([4.0,12.0])\n",
    "    plt.ylim([4.0,12.0])\n",
    "    plt.xlabel('HbA1c Invasive device (%NGSP)',fontsize=15)\n",
    "    plt.ylabel('HbA1c Estimated (%NGSP)',fontsize=15)\n",
    "    plt.title('Error Grid Analysis (EGA) for estimated %HbA1c values',fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics_including_BA_and_EGA_plot(modelName, predictedValue, exactValue):\n",
    "    print('############ HbA1c Evaluation Metrics of ' + modelName + ' Regression ############')\n",
    "    CVy = np.asarray(predictedValue)\n",
    "    CVx = np.asarray(exactValue)\n",
    "    print('Diff STD  = ',np.std(CVx-CVy))\n",
    "    print('MSE       = ',np.sum((CVx-CVy)**2)/CVy.shape[0])\n",
    "    print('ME        = ',np.sum((CVx-CVy))/CVy.shape[0])\n",
    "    print('MAD       = ',np.sum(np.abs(CVx-CVy))/CVy.shape[0])\n",
    "    print('RMSE      = ',np.sqrt(np.sum((CVx-CVy)**2)/CVy.shape[0]))\n",
    "    print('R2 score  = ',r2_score(CVx,CVy))\n",
    "    print('Pearson R and p-value: ',scipy.stats.pearsonr(CVx,CVy))\n",
    "    print('##################################################################################\\n')\n",
    "    bland_altman_plot  (predictedValue, exactValue)\n",
    "    error_grid_analysis(predictedValue, exactValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize Second Order Section\n",
    "sos  = scipy.signal.butter(N=2, Wn=8.0, btype='low',  fs=32, output='sos')  ### The sampling frequency of the digital system(‘fs’).second-order sections (‘sos’)\n",
    "sos1 = scipy.signal.butter(N=2, Wn=0.5, btype='high', fs=32, output='sos')  ###\n",
    "def removing_baseline_drift (ppgSignal, showFigure='False'): \n",
    "   # Getting corresponding Combined PPG Data \n",
    "   dat   = pd.read_csv(ppgSignal, delimiter=',', header=None)\n",
    "    \n",
    "   bs = dat.iloc[:,2]\n",
    "   gs = dat.iloc[:,1]\n",
    "   rs = dat.iloc[:,0]\n",
    "  \n",
    "   fil_s_b = scipy.signal.sosfilt(sos, bs)[10:]\n",
    "   fil_s_g = scipy.signal.sosfilt(sos, gs)[10:]\n",
    "   fil_s_r = scipy.signal.sosfilt(sos, rs)[10:]\n",
    "\n",
    "   fil_s_b1 = scipy.signal.sosfilt(sos1, fil_s_b)[110:] + np.mean(fil_s_b)\n",
    "   fil_s_g1 = scipy.signal.sosfilt(sos1, fil_s_g)[110:] + np.mean(fil_s_g)\n",
    "   fil_s_r1 = scipy.signal.sosfilt(sos1, fil_s_r)[110:] + np.mean(fil_s_r)\n",
    "   \n",
    "   ### Visualization\n",
    "   if showFigure=='True':\n",
    "      print('***************Blue**************')\n",
    "      plt.rcParams.update(plt.rcParamsDefault)\n",
    "      plt.rcParams['font.size'] = '9'\n",
    "      fig,axs = plt.subplots(2, figsize=(20, 2.5))\n",
    "      axs[0].set(title='Visualization of PPG Signal before and after filtering')\n",
    "      axs[0].plot(pd.DataFrame(fil_s_b), color='blue', label='after first filtering')\n",
    "      axs[0].set(ylabel = ' ', xlabel = ' ')\n",
    "      axs[0].legend(loc=1)\n",
    "      axs[0].grid(True)\n",
    "      axs[1].plot(pd.DataFrame(fil_s_b1), color='blue', label='After 2nd filter and baseline_drift_removal')\n",
    "      axs[1].set(ylabel = ' ', xlabel = ' ')\n",
    "      axs[1].legend(loc=1)\n",
    "      axs[1].grid(True)\n",
    "      plt.show()\n",
    "\n",
    "      print('***************Green**************')\n",
    "      plt.rcParams.update(plt.rcParamsDefault)\n",
    "      plt.rcParams['font.size'] = '9'\n",
    "      fig,axs = plt.subplots(2, figsize=(20, 2.5))\n",
    "      axs[0].set(title='Visualization of PPG Signal before and after filtering')\n",
    "      axs[0].plot(pd.DataFrame(fil_s_g), color='green', label='after first filtering')\n",
    "      axs[0].set(ylabel = ' ', xlabel = ' ')\n",
    "      axs[0].legend(loc=1)\n",
    "      axs[0].grid(True)\n",
    "      axs[1].plot(pd.DataFrame(fil_s_g1), color='green', label='After 2nd filter and baseline_drift_removal')\n",
    "      axs[1].set(ylabel = ' ', xlabel = ' ')\n",
    "      axs[1].legend(loc=1)\n",
    "      axs[1].grid(True)\n",
    "      plt.show()\n",
    "      \n",
    "      print('***************Red**************')\n",
    "      plt.rcParams.update(plt.rcParamsDefault)\n",
    "      plt.rcParams['font.size'] = '9'\n",
    "      fig,axs = plt.subplots(2, figsize=(20, 2.5))\n",
    "      axs[0].set(title='Visualization of PPG Signal before and after filtering')\n",
    "      axs[0].plot(pd.DataFrame(fil_s_r), color='red', label='after first filtering')\n",
    "      axs[0].set(ylabel = ' ', xlabel = ' ')\n",
    "      axs[0].legend(loc=1)\n",
    "      axs[0].grid(True)\n",
    "      axs[1].plot(pd.DataFrame(fil_s_r1), color='red', label='After 2nd filter and baseline_drift_removal')\n",
    "      axs[1].set(ylabel = ' ', xlabel = ' ')\n",
    "      axs[1].legend(loc=1)\n",
    "      axs[1].grid(True)\n",
    "      plt.show()\n",
    "\n",
    "   ## Return outcome\n",
    "   return(fil_s_r1, fil_s_g1, fil_s_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,g,b = removing_baseline_drift(ppgSignal=PPG_csvArray [14], showFigure='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSignal (individualSignal, horizonSize=50, windowSize=200):\n",
    "    overallSignal  = []\n",
    "    completeFlag   = 0\n",
    "    for i in range (0, len(individualSignal), horizonSize):\n",
    "        if i+windowSize < len(individualSignal): #Only consider sets with windowSize data\n",
    "            splittedSignal = []\n",
    "            for j in range(windowSize):\n",
    "                splittedSignal.append(individualSignal[i+j])\n",
    "            overallSignal.append(splittedSignal)\n",
    "    x = np.array(overallSignal) \n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horizonSet = 7*5\n",
    "horizonSet = 32\n",
    "#windowSet  = 7*13\n",
    "windowSet  = 96 #72\n",
    "countData  = 0\n",
    "for i in range(len(PPG_csvArray)):\n",
    "    r,g,b = removing_baseline_drift(ppgSignal=PPG_csvArray[i], showFigure='False')\n",
    "    r_ = splitSignal(individualSignal=r, horizonSize=horizonSet, windowSize=windowSet)\n",
    "    print(r_.shape)\n",
    "    countData += r_.shape[0]*r_.shape[1]\n",
    "print(f'dCount={countData}')\n",
    "### Plotting on the first one\n",
    "pd.DataFrame(r_[0]).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_filter(xArray, cutoffFreq=2, samplingFreq=24, filterOrder=4, filterType='low'):\n",
    "    nyquistFreq  = 0.5 * samplingFreq\n",
    "    criticalFreq = cutoffFreq/nyquistFreq\n",
    "    # Butterworth digital and analog filter design.\n",
    "    if filterType == 'band':\n",
    "        sos = scipy.signal.butter(N=filterOrder, Wn=[0.1, criticalFreq], btype='bandpass', analog=False, output='sos')\n",
    "    elif filterType == 'low':\n",
    "        sos = scipy.signal.butter(N=filterOrder, Wn=criticalFreq,        btype='lowpass',  analog=False, output='sos')\n",
    "    elif filterType == 'high':\n",
    "        sos = scipy.signal.butter(N=filterOrder, Wn=criticalFreq,        btype='highpass', analog=False, output='sos')\n",
    "    # Filter data along one-dimension with an IIR or FIR filter.\n",
    "    y = scipy.signal.sosfilt(sos=sos, x=xArray)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual Splitted Signal')\n",
    "pd.DataFrame(r_[0]).plot()\n",
    "plt.show()\n",
    "print('After applying mean function')\n",
    "data = r_[0]-np.mean(r_[0])\n",
    "pd.DataFrame(data).plot()\n",
    "plt.show()\n",
    "print('After applying filter')\n",
    "filteredSignal_lp = butter_filter(xArray=data, cutoffFreq=2, samplingFreq=24, filterOrder=4, filterType='low')\n",
    "filteredSignal_bp = butter_filter(xArray=data, cutoffFreq=8, samplingFreq=24, filterOrder=2, filterType='band')\n",
    "filteredSignal_hp = butter_filter(xArray=data, cutoffFreq=8, samplingFreq=24, filterOrder=2, filterType='high')\n",
    "pd.DataFrame(filteredSignal_lp).plot()\n",
    "pd.DataFrame(filteredSignal_bp).plot()\n",
    "pd.DataFrame(filteredSignal_hp).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_ac_dc_from_PPG (Signal, colorSignal, showFigure='False'):\n",
    "    maxArray = []\n",
    "    minArray = []\n",
    "    p2pPanDF = pd.DataFrame({'index':[], 'value':[]})\n",
    "    midValue = np.mean(Signal)\n",
    "    maxValue = midValue\n",
    "    minValue = midValue\n",
    "    maxIndex = 0\n",
    "    minIndex = 0\n",
    "\n",
    "    for i in range(len(Signal)):\n",
    "        nowValue = Signal[i]\n",
    "        if (nowValue>midValue):\n",
    "            if (minValue<midValue and minIndex>0):\n",
    "                p2pPanDF = pd.concat([p2pPanDF, pd.DataFrame({'index':[minIndex], 'value':[minValue]})], ignore_index=True, sort=False)\n",
    "                minArray = np.append(minArray, minValue)\n",
    "                minValue = midValue\n",
    "            if (nowValue>maxValue):\n",
    "                maxValue = nowValue\n",
    "                maxIndex = i\n",
    "        if (nowValue<midValue):\n",
    "            if (maxValue>midValue and maxIndex>0):\n",
    "                p2pPanDF = pd.concat([p2pPanDF, pd.DataFrame({'index':[maxIndex], 'value':[maxValue]})], ignore_index=True, sort=False)\n",
    "                maxArray = np.append(maxArray, maxValue)\n",
    "                maxValue = midValue\n",
    "            if (nowValue<minValue):\n",
    "                minValue = nowValue\n",
    "                minIndex = i\n",
    "\n",
    "    ## Calculate AC and DC\n",
    "    acSignal = np.mean(maxArray) - np.mean(minArray)\n",
    "    dcSignal = np.mean(minArray)\n",
    "\n",
    "    ## Show Figure\n",
    "    if (showFigure=='True'):\n",
    "        plt.rcParams.update(plt.rcParamsDefault)\n",
    "        plt.rcParams['font.size'] = '9'\n",
    "        plt.figure(figsize=(21, 3))\n",
    "        plt.title(f'***Analyze AC and DC Characteristics in a PPG Signal*** \\n AC={acSignal} and DC={dcSignal} \\n Total Peaks={len(maxArray)}(mean={np.mean(maxArray)}) and Valley={len(minArray)}(mean={np.mean(minArray)})', y=1, loc='center')\n",
    "        plt.xlabel('Sample Count')\n",
    "        plt.ylabel(' ')\n",
    "        for i in range(len(Signal)):\n",
    "            plt.plot(i, midValue,          '.', color='k')\n",
    "            plt.plot(i, np.mean(maxArray), '.', color='m')\n",
    "            plt.plot(i, np.mean(minArray), '.', color='y')\n",
    "        plt.plot(Signal, color=f'{colorSignal}')\n",
    "        plt.plot(p2pPanDF.values[:,0], p2pPanDF.values[:,1], 'x', color='k')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    #return acSignal, dcSignal, maxArray,minArray,p2pPanDF\n",
    "    return acSignal, dcSignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kaiser_teager_energy(x):\n",
    "    x         = np.array(x)\n",
    "    l         = len(x)\n",
    "    squ       = x[1:l-1]**2\n",
    "    oddi      = x[0:l-2]\n",
    "    eveni     = x[2:l]\n",
    "    ex        = squ - (oddi*eveni)\n",
    "    kte       = np.zeros([l,])\n",
    "    kte[0]    = ex[0]\n",
    "    kte[-1]   = ex[-1]\n",
    "    kte[1:-1] = ex\n",
    "    return kte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feature_set_v1 (PPG, HbA1c, SpO2, BMI, samplingRate=32):\n",
    "    # Initialization\n",
    "    sr         = samplingRate\n",
    "    l          = 0\n",
    "\n",
    "    for i in range (0,len(PPG)):\n",
    "        \n",
    "        # Getting corresponding PPG Data\n",
    "        # > __Algorithm 01:__ Baseline Dirft Removal by () \n",
    "        r,g,b = removing_baseline_drift(ppgSignal=PPG[i], showFigure='False')\n",
    "        print('PPG_data_info:'+str(PPG[i]))\n",
    "\n",
    "        # > __Algorithm 04:__ Determine AC_DC_from_Signal\n",
    "        r_ac,r_dc = determine_ac_dc_from_PPG(r,colorSignal='r',showFigure='False')\n",
    "        g_ac,g_dc = determine_ac_dc_from_PPG(g,colorSignal='g',showFigure='False')\n",
    "        b_ac,b_dc = determine_ac_dc_from_PPG(b, colorSignal='b',showFigure='False')\n",
    "\n",
    "        # Calculate r1 & r2\n",
    "        r1 = (g_ac/g_dc)/(r_ac/r_dc)\n",
    "        r2 = (b_ac/b_dc)/(r_ac/r_dc)\n",
    "        r3 = (g_ac/g_dc)/(b_ac/b_dc)\n",
    "        r4 = (g_ac/g_dc)\n",
    "        r5 = (b_ac/b_dc)\n",
    "        r6 = (r_ac/r_dc)\n",
    "\n",
    "        # > __Algorithm 02:__ Split_Signal\n",
    "        r_sp = splitSignal(individualSignal=r, horizonSize=horizonSet, windowSize=windowSet)\n",
    "        g_sp = splitSignal(individualSignal=g, horizonSize=horizonSet, windowSize=windowSet)\n",
    "        b_sp = splitSignal(individualSignal=b, horizonSize=horizonSet, windowSize=windowSet)\n",
    "        \n",
    "        # Constructing Feature Set\n",
    "        for j in range(len(r_sp)):  ### You can also use r_sp.shape[0]\n",
    "            # Initialize frame\n",
    "            featureFrame       = []\n",
    "            \n",
    "            # >>>>>>>>>>>>>>>>Red>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            # > __Algorithm 03:__ Filter\n",
    "            filteredSignal_r = butter_filter(xArray=r_sp[j]-np.mean(r_sp[j]), cutoffFreq=8, samplingFreq=sr, filterOrder=2, filterType='low')\n",
    "\n",
    "            fqSignal_r, psdSignal_r    = scipy.signal.welch(\n",
    "                x       = filteredSignal_r, \n",
    "                nperseg = len(filteredSignal_r) # Added to overcome error\n",
    "                ) \n",
    "\n",
    "\n",
    "            # > __Algorithm 05:__ Kaiser–Teager Energy\n",
    "            kteSignal_r            = get_kaiser_teager_energy(\n",
    "                filteredSignal_r\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "            # Insert Feature one by one\n",
    "            # Add new feature (5)\n",
    "            ar_coeffs_r, _r        = statsmodels.regression.linear_model.yule_walker(filteredSignal_r, order = 2)\n",
    "            #featureFrame           = np.append(featureFrame,ar_coeffs_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            zcSignal_r             = tsfel.feature_extraction.features.zero_cross(filteredSignal_r)\n",
    "            #featureFrame           = np.append(featureFrame,zcSignal_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            skewSignal_r           = tsfel.feature_extraction.features.skewness(filteredSignal_r)\n",
    "            #featureFrame           = np.append(featureFrame,skewSignal_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            absWavelength_r        = tsfel.feature_extraction.wavelet_abs_mean(filteredSignal_r)\n",
    "            maWavelength_r         = np.mean(absWavelength_r)\n",
    "            #featureFrame           = np.append(featureFrame,maWavelength_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            autocorr_r             = tsfel.feature_extraction.autocorr(filteredSignal_r)\n",
    "            #featureFrame           = np.append(featureFrame,autocorr_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            kurtSpectral_r         = tsfel.feature_extraction.features.spectral_kurtosis(filteredSignal_r, sr)\n",
    "            #featureFrame           = np.append(featureFrame,kurtSpectral_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            skewSpectral_r         = tsfel.feature_extraction.features.spectral_skewness(filteredSignal_r, sr)\n",
    "            #featureFrame           = np.append(featureFrame,skewSpectral_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            sum_abs_diff_r         = tsfel.feature_extraction.features.sum_abs_diff(filteredSignal_r)\n",
    "            #featureFrame           = np.append(featureFrame,sum_abs_diff_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            kurtPSD_r              = scipy.stats.kurtosis(psdSignal_r)\n",
    "            #featureFrame           = np.append(featureFrame,kurtPSD_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            varPSD_r               = np.var(psdSignal_r)\n",
    "            #featureFrame           = np.append(featureFrame,varPSD_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            meanPSD_r              = np.mean(psdSignal_r)\n",
    "            #featureFrame           = np.append(featureFrame,meanPSD_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            skewKTE_r              = scipy.stats.skew(kteSignal_r)\n",
    "            #featureFrame           = np.append(featureFrame,skewKTE_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            kurtKTE_r              = scipy.stats.kurtosis(kteSignal_r)\n",
    "            #featureFrame           = np.append(featureFrame,kurtKTE_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            varKTE_r               = np.var(kteSignal_r)\n",
    "            #featureFrame           = np.append(featureFrame,varKTE_r)\n",
    "            \n",
    "            # Add new feature\n",
    "            meanKTE_r              = np.mean(kteSignal_r)\n",
    "            #featureFrame           = np.append(featureFrame,meanKTE_r)\n",
    "            \n",
    "            \n",
    "            # >>>>>>>>>>>>>>>>Green>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "           \n",
    "            # > __Algorithm 03:__ Butter Low-pass Filter\n",
    "            filteredSignal_g = butter_filter(xArray=g_sp[j]-np.mean(g_sp[j]), cutoffFreq=8, samplingFreq=sr, filterOrder=2, filterType='low')\n",
    "\n",
    "\n",
    "            fqSignal_g, psdSignal_g    = scipy.signal.welch(\n",
    "                x       = filteredSignal_g, \n",
    "                nperseg = len(filteredSignal_g) # Added to overcome error\n",
    "                ) \n",
    "           \n",
    "            # > __Algorithm 05:__ Kaiser–Teager Energy\n",
    "            kteSignal_g            = get_kaiser_teager_energy(\n",
    "                filteredSignal_g\n",
    "                )\n",
    "            \n",
    "            \n",
    "            # Insert Feature one by one\n",
    "            # Add new feature\n",
    "            ar_coeffs_g, _g        = statsmodels.regression.linear_model.yule_walker(filteredSignal_g, order = 2)\n",
    "            #featureFrame           = np.append(featureFrame,ar_coeffs_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            zcSignal_g             = tsfel.feature_extraction.features.zero_cross(filteredSignal_g)\n",
    "            #featureFrame           = np.append(featureFrame,zcSignal_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            skewSignal_g           = tsfel.feature_extraction.features.skewness(filteredSignal_g)\n",
    "            featureFrame           = np.append(featureFrame,skewSignal_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            absWavelength_g        = tsfel.feature_extraction.wavelet_abs_mean(filteredSignal_g)\n",
    "            maWavelength_g         = np.mean(absWavelength_g)\n",
    "            #featureFrame           = np.append(featureFrame,maWavelength_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            autocorr_g             = tsfel.feature_extraction.autocorr(filteredSignal_g)\n",
    "            featureFrame           = np.append(featureFrame,autocorr_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            kurtSpectral_g         = tsfel.feature_extraction.features.spectral_kurtosis(filteredSignal_g, sr)\n",
    "            #featureFrame           = np.append(featureFrame,kurtSpectral_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            skewSpectral_g         = tsfel.feature_extraction.features.spectral_skewness(filteredSignal_g, sr)\n",
    "            #featureFrame           = np.append(featureFrame,skewSpectral_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            sum_abs_diff_g         = tsfel.feature_extraction.features.sum_abs_diff(filteredSignal_g)\n",
    "            #featureFrame           = np.append(featureFrame,sum_abs_diff_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            kurtPSD_g              = scipy.stats.kurtosis(psdSignal_g)\n",
    "            featureFrame           = np.append(featureFrame,kurtPSD_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            varPSD_g               = np.var(psdSignal_g)\n",
    "            featureFrame           = np.append(featureFrame,varPSD_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            meanPSD_g              = np.mean(psdSignal_g)\n",
    "            #featureFrame           = np.append(featureFrame,meanPSD_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            skewKTE_g              = scipy.stats.skew(kteSignal_g)\n",
    "            #featureFrame           = np.append(featureFrame,skewKTE_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            kurtKTE_g              = scipy.stats.kurtosis(kteSignal_g)\n",
    "            #featureFrame           = np.append(featureFrame,kurtKTE_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            varKTE_g               = np.var(kteSignal_g)\n",
    "            #featureFrame           = np.append(featureFrame,varKTE_g)\n",
    "            \n",
    "            # Add new feature\n",
    "            meanKTE_g              = np.mean(kteSignal_g)\n",
    "            #featureFrame           = np.append(featureFrame,meanKTE_g)\n",
    "            \n",
    "            # >>>>>>>>>>>>>>>>Blue>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "            # > __Algorithm 03:__ Butter Low-pass Filter\n",
    "            filteredSignal_b = butter_filter(xArray=b_sp[j]-np.mean(b_sp[j]), cutoffFreq=8, samplingFreq=sr, filterOrder=2, filterType='low')\n",
    "\n",
    "            fqSignal_b, psdSignal_b    = scipy.signal.welch(\n",
    "                x       = filteredSignal_b, \n",
    "                nperseg = len(filteredSignal_b) # Added to overcome error\n",
    "                ) \n",
    "           \n",
    "            # > __Algorithm 05:__ Kaiser–Teager Energy\n",
    "            kteSignal_b            = get_kaiser_teager_energy(\n",
    "                filteredSignal_b\n",
    "                )\n",
    "            # Insert Feature one by one\n",
    "            # Add new feature\n",
    "            ar_coeffs_b, _b        = statsmodels.regression.linear_model.yule_walker(filteredSignal_b, order = 2)\n",
    "            #featureFrame           = np.append(featureFrame,ar_coeffs_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            zcSignal_b             = tsfel.feature_extraction.features.zero_cross(filteredSignal_b)\n",
    "            #featureFrame           = np.append(featureFrame,zcSignal_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            skewSignal_b           = tsfel.feature_extraction.features.skewness(filteredSignal_b)\n",
    "            #featureFrame           = np.append(featureFrame,skewSignal_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            absWavelength_b        = tsfel.feature_extraction.wavelet_abs_mean(filteredSignal_b)\n",
    "            maWavelength_b         = np.mean(absWavelength_b)\n",
    "            #featureFrame           = np.append(featureFrame,maWavelength_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            autocorr_b             = tsfel.feature_extraction.autocorr(filteredSignal_b)\n",
    "            #featureFrame           = np.append(featureFrame,autocorr_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            kurtSpectral_b         = tsfel.feature_extraction.features.spectral_kurtosis(filteredSignal_b, sr)\n",
    "            #featureFrame           = np.append(featureFrame,kurtSpectral_b)\n",
    "\n",
    "            # Add new feature\n",
    "            skewSpectral_b         = tsfel.feature_extraction.features.spectral_skewness(filteredSignal_b, sr)\n",
    "            #featureFrame           = np.append(featureFrame,skewSpectral_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            sum_abs_diff_b         = tsfel.feature_extraction.features.sum_abs_diff(filteredSignal_b)\n",
    "            #featureFrame           = np.append(featureFrame,sum_abs_diff_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            kurtPSD_b              = scipy.stats.kurtosis(psdSignal_b)\n",
    "            #featureFrame           = np.append(featureFrame,kurtPSD_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            varPSD_b               = np.var(psdSignal_b)\n",
    "            #featureFrame           = np.append(featureFrame,varPSD_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            meanPSD_b              = np.mean(psdSignal_b)\n",
    "            #featureFrame           = np.append(featureFrame,meanPSD_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            skewKTE_b              = scipy.stats.skew(kteSignal_b)\n",
    "            #featureFrame           = np.append(featureFrame,skewKTE_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            kurtKTE_b              = scipy.stats.kurtosis(kteSignal_b)\n",
    "            #featureFrame           = np.append(featureFrame,kurtKTE_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            varKTE_b               = np.var(kteSignal_b)\n",
    "            #featureFrame           = np.append(featureFrame,varKTE_b)\n",
    "            \n",
    "            # Add new feature\n",
    "            meanKTE_b              = np.mean(kteSignal_b)\n",
    "            #featureFrame           = np.append(featureFrame,meanKTE_b)\n",
    "\n",
    "            # >>>>>>>>>>>>>>>>Add_r1_r2_as_features>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            # filteredSignal_b = butter_lowPass_filter(\n",
    "            #     data  = b_sp[j]-np.mean(b_sp[j]), \n",
    "            #     cutoff= 8, \n",
    "            #     fs    = sr, \n",
    "            #     order = 6\n",
    "            #     )\n",
    "\n",
    "            \n",
    "            featureFrame           = np.append(featureFrame,r4) # green ac dc\n",
    "            #featureFrame           = np.append(featureFrame,r5) # blue ac dc \n",
    "            #featureFrame           = np.append(featureFrame,r6) # red ac dc \n",
    "            #featureFrame           = np.append(featureFrame,r2)\n",
    "            #featureFrame           = np.append(featureFrame,r3)\n",
    "\n",
    "            # >>>>>>>>>>>>>>>>Add_other_features>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            featureFrame           = np.append(featureFrame,SpO2[i])\n",
    "            featureFrame           = np.append(featureFrame,BMI[i])\n",
    "\n",
    "            # >>>>>>>>>>>>>>> Add output feature \n",
    "            featureFrame           = np.append(featureFrame,HbA1c[i])\n",
    "\n",
    "            ## Insert featureFrame into featureSet\n",
    "            # Initialized feature set array \n",
    "            if l == 0:\n",
    "                k = len(featureFrame)\n",
    "                # print('kCount='+str(k))\n",
    "                featureSet = np.empty([150000,k]) \n",
    "            # Save into featureSet\n",
    "            featureSet[l][:k] = featureFrame\n",
    "            # Length increased for next cycle\n",
    "            l = l + 1\n",
    "\n",
    "\n",
    "            # Checking outputs\n",
    "            print(f'iCount={i} || jCount={j} || lCount={l} || fCount={len(featureFrame)}')\n",
    "            print(featureFrame)\n",
    "\n",
    "        # Check point    \n",
    "        # print('iCount='+str(i))\n",
    "\n",
    "    # Gather all\n",
    "    print('**********************************************************************')\n",
    "    print('*****************************Summary**********************************')    \n",
    "    featureSet = featureSet[0:l][:]\n",
    "    # Check for null\n",
    "    featureSet = featureSet[~np.isnan(featureSet).any(axis=1)]\n",
    "    print('**********************************************************************')\n",
    "    return featureSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "loo                = sk.model_selection.LeaveOneOut()\n",
    "t1_scores          = []\n",
    "t1_test_cases      = []\n",
    "t1_train_cases     = []\n",
    "countEpoch         = 0\n",
    "percent            = 0.02\n",
    "# XG Boost\n",
    "t1_results_xg      = []\n",
    "t1_e_results_xg    = []\n",
    "# Random Forest\n",
    "t1_results_rf      = []\n",
    "t1_e_results_rf    = []\n",
    "# Cat Boost\n",
    "t1_results_cb      = []\n",
    "t1_e_results_cb    = []\n",
    "# Light GBM\n",
    "t1_results_lg      = []\n",
    "t1_e_results_lg    = []\n",
    "\n",
    "for train_index, test_index in loo.split(PPG_csvArray):\n",
    "    print('Epoch # '+str(countEpoch))\n",
    "    countEpoch +=1\n",
    "\n",
    "    X_train, X_test      = PPG_csvArray[train_index], PPG_csvArray[test_index]\n",
    "    y_train, y_test      = HbA1c_Array [train_index], HbA1c_Array [test_index]\n",
    "    sp_train, sp_test    = SpO2_Array  [train_index], SpO2_Array  [test_index]\n",
    "    bmi_train, bmi_test  = BMI_Array   [train_index], BMI_Array   [test_index]\n",
    "    \n",
    "    \n",
    "    # > __Algorithm 06v1:__ Constructing Features Set _(Red, Green, Blue; Reflected, FeatureCount=[4+4+r1+r2]+[HbA1c]=11)_\n",
    "    print('*************************************************************')\n",
    "    print('*********************Training Array Set**********************')\n",
    "    print('*************************************************************')\n",
    "    train_array = construct_feature_set_v1(\n",
    "        PPG          = X_train,\n",
    "        HbA1c        = y_train,\n",
    "        SpO2         = sp_train,\n",
    "        BMI          = bmi_train,\n",
    "        samplingRate = 24\n",
    "        )\n",
    "    print('*************************************************************')\n",
    "    print('*********************Testing Array Set***********************')\n",
    "    print('*************************************************************')\n",
    "    test_array  = construct_feature_set_v1(\n",
    "        PPG          = X_test,\n",
    "        HbA1c        = y_test,\n",
    "        SpO2         = sp_test,\n",
    "        BMI          = bmi_test,\n",
    "        samplingRate = 24\n",
    "        )\n",
    "    \n",
    "    feature_train, hba1c_train = train_array[:,:-1], train_array[:,-1]\n",
    "    feature_test, hba1c_test   = test_array[:,:-1],  test_array[:,-1]\n",
    "    \n",
    "    train = np.sort(np.array(random.sample(range(feature_test.shape[0]), int(feature_test.shape[0] * percent))))\n",
    "    if train.shape[0] == 0:\n",
    "        feature_train, hba1c_train = np.append(feature_train, np.expand_dims(feature_test[0],axis=0), 0), np.append(hba1c_train, np.expand_dims(hba1c_test[0],axis=0), 0)\n",
    "        # feature_test, hba1c_test = np.delete(feature_test, 0, axis = 0), np.delete(hba1c_test, 0, axis = 0)\n",
    "    else:\n",
    "        test = np.delete(np.arange(feature_test.shape[0]), train)\n",
    "\n",
    "        feature_train, hba1c_train = np.append(feature_train, feature_test[train], 0), np.append(hba1c_train, hba1c_test[train], 0)\n",
    "        feature_test, hba1c_test = np.delete(feature_test, train, axis = 0), np.delete(hba1c_test, train, axis = 0)\n",
    "    \n",
    "    feature_train, hba1c_train = sk.utils.shuffle(feature_train, hba1c_train, random_state=0)\n",
    "    # print(feature_train.shape)\n",
    "    # print(feature_test.shape)\n",
    "\n",
    "    # XGboost\n",
    "    #xg = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42, n_estimators= 1000, eta=0.05, subsample=0.5, colsample_bytree=0.5)\n",
    "    xg = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42, n_estimators=100, eta=0.1, max_depth=2, subsample=0.8, colsample_bytree=0.8)\n",
    "    xg.fit(feature_train, hba1c_train)\n",
    "    t1_results_xg.append(np.nanmean(xg.predict(feature_test)))\n",
    "    t1_e_results_xg.append(np.nanmean(hba1c_test))\n",
    "\n",
    "    # Random Forest\n",
    "    #rf = RandomForestRegressor(random_state=42, n_estimators=1000, n_jobs=-1)\n",
    "    #rf = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=3, min_samples_leaf=3, min_samples_split=3, n_jobs=-1)\n",
    "    rf = RandomForestRegressor(random_state=42, n_estimators=150, max_depth=4, min_samples_leaf=2, min_samples_split=2, n_jobs=-1)\n",
    "    rf.fit(feature_train, hba1c_train)\n",
    "    t1_results_rf.append(np.nanmean(rf.predict(feature_test)))\n",
    "    t1_e_results_rf.append(np.nanmean(hba1c_test))\n",
    "\n",
    "    # Cat boost\n",
    "    #cb = cab.CatBoostRegressor(depth=3, l2_leaf_reg=5)\n",
    "    #cb = cab.CatBoostRegressor(depth=2, l2_leaf_reg=3, learning_rate=0.1, iterations=100, loss_function='RMSE', random_seed=42, verbose=False)\n",
    "    cb = cab.CatBoostRegressor(depth=2, l2_leaf_reg=3, learning_rate=0.1, iterations=200, loss_function='RMSE', random_seed=42, verbose=False)\n",
    "    cb.fit(feature_train, hba1c_train)\n",
    "    t1_results_cb.append(np.nanmean(cb.predict(feature_test)))\n",
    "    t1_e_results_cb.append(np.nanmean(hba1c_test))\n",
    "\n",
    "    # Light GBM\n",
    "    # lg = ltb.LGBMRegressor(boosting_type='dart')\n",
    "    #lg = ltb.LGBMRegressor(\n",
    "    #application='regression',\n",
    "    #objective='root_mean_squared_error',\n",
    "    #boosting_type='gbdt',\n",
    "    #learning_rate=0.05,\n",
    "    #num_leaves=31,\n",
    "    #max_depth=-1,\n",
    "    #min_data_in_leaf=5,\n",
    "    #metric='rmse',\n",
    "    #feature_fraction=0.8,\n",
    "    #bagging_fraction=0.8,\n",
    "    #bagging_freq=5,\n",
    "    #random_state=42,\n",
    "    #n_estimators=500\n",
    "    #)\n",
    "\n",
    "    lg = ltb.LGBMRegressor(\n",
    "    application='regression',\n",
    "    objective='root_mean_squared_error',\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=15,\n",
    "    max_depth=2,\n",
    "    min_data_in_leaf=3,\n",
    "    metric='rmse',\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=5,\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    "    )\n",
    "    \n",
    "    lg.fit(feature_train, hba1c_train)\n",
    "    t1_results_lg.append(np.nanmean(lg.predict(feature_test)))\n",
    "    t1_e_results_lg.append(np.nanmean(hba1c_test))\n",
    "\n",
    "\n",
    "\n",
    "    t1_test_cases.append(X_test)\n",
    "    t1_train_cases.append(X_train)\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "loo                = sk.model_selection.LeaveOneOut()\n",
    "t1_scores          = []\n",
    "t1_test_cases      = []\n",
    "t1_train_cases     = []\n",
    "countEpoch         = 0\n",
    "percent            = 0.02\n",
    "# XG Boost\n",
    "t1_results_xg      = []\n",
    "t1_e_results_xg    = []\n",
    "# Random Forest\n",
    "t1_results_rf      = []\n",
    "t1_e_results_rf    = []\n",
    "# Cat Boost\n",
    "t1_results_cb      = []\n",
    "t1_e_results_cb    = []\n",
    "# Light GBM\n",
    "t1_results_lg      = []\n",
    "t1_e_results_lg    = []\n",
    "\n",
    "for train_index, test_index in loo.split(PPG_csvArray):\n",
    "    print('Epoch # '+str(countEpoch))\n",
    "    countEpoch +=1\n",
    "\n",
    "    X_train, X_test      = PPG_csvArray[train_index], PPG_csvArray[test_index]\n",
    "    y_train, y_test      = HbA1c_Array [train_index], HbA1c_Array [test_index]\n",
    "    sp_train, sp_test    = SpO2_Array  [train_index], SpO2_Array  [test_index]\n",
    "    bmi_train, bmi_test  = BMI_Array   [train_index], BMI_Array   [test_index]\n",
    "    \n",
    "    \n",
    "    # > __Algorithm 06v1:__ Constructing Features Set _(Red, Green, Blue; Reflected, FeatureCount=[4+4+r1+r2]+[HbA1c]=11)_\n",
    "    print('*************************************************************')\n",
    "    print('*********************Training Array Set**********************')\n",
    "    print('*************************************************************')\n",
    "    train_array = construct_feature_set_v1(\n",
    "        PPG          = X_train,\n",
    "        HbA1c        = y_train,\n",
    "        SpO2         = sp_train,\n",
    "        BMI          = bmi_train,\n",
    "        samplingRate = 24\n",
    "        )\n",
    "    print('*************************************************************')\n",
    "    print('*********************Testing Array Set***********************')\n",
    "    print('*************************************************************')\n",
    "    test_array  = construct_feature_set_v1(\n",
    "        PPG          = X_test,\n",
    "        HbA1c        = y_test,\n",
    "        SpO2         = sp_test,\n",
    "        BMI          = bmi_test,\n",
    "        samplingRate = 24\n",
    "        )\n",
    "    \n",
    "    feature_train, hba1c_train = train_array[:,:-1], train_array[:,-1]\n",
    "    feature_test, hba1c_test   = test_array[:,:-1],  test_array[:,-1]\n",
    "    \n",
    "    train = np.sort(np.array(random.sample(range(feature_test.shape[0]), int(feature_test.shape[0] * percent))))\n",
    "    if train.shape[0] == 0:\n",
    "        feature_train, hba1c_train = np.append(feature_train, np.expand_dims(feature_test[0],axis=0), 0), np.append(hba1c_train, np.expand_dims(hba1c_test[0],axis=0), 0)\n",
    "        # feature_test, hba1c_test = np.delete(feature_test, 0, axis = 0), np.delete(hba1c_test, 0, axis = 0)\n",
    "    else:\n",
    "        test = np.delete(np.arange(feature_test.shape[0]), train)\n",
    "\n",
    "        feature_train, hba1c_train = np.append(feature_train, feature_test[train], 0), np.append(hba1c_train, hba1c_test[train], 0)\n",
    "        feature_test, hba1c_test = np.delete(feature_test, train, axis = 0), np.delete(hba1c_test, train, axis = 0)\n",
    "    \n",
    "    feature_train, hba1c_train = sk.utils.shuffle(feature_train, hba1c_train, random_state=0)\n",
    "    # print(feature_train.shape)\n",
    "    # print(feature_test.shape)\n",
    "\n",
    "    # XGboost\n",
    "    #xg = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42, n_estimators= 1000, eta=0.05, subsample=0.5, colsample_bytree=0.5)\n",
    "    xg = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42, n_estimators=200, eta=0.03, max_depth=3,  subsample=0.5, colsample_bytree=0.5)\n",
    "    xg.fit(feature_train, hba1c_train)\n",
    "    t1_results_xg.append(np.nanmean(xg.predict(feature_test)))\n",
    "    t1_e_results_xg.append(np.nanmean(hba1c_test))\n",
    "\n",
    "    # Random Forest\n",
    "    #rf = RandomForestRegressor(random_state=42, n_estimators=1000, n_jobs=-1)\n",
    "    rf = RandomForestRegressor(random_state=42, n_estimators=200, max_depth=3, min_samples_leaf=3, min_samples_split=3, n_jobs=-1)\n",
    "    #rf = RandomForestRegressor(random_state=42,max_depth=8, n_jobs=-1)\n",
    "    rf.fit(feature_train, hba1c_train)\n",
    "    t1_results_rf.append(np.nanmean(rf.predict(feature_test)))\n",
    "    t1_e_results_rf.append(np.nanmean(hba1c_test))\n",
    "\n",
    "    # Cat boost\n",
    "    #cb = cab.CatBoostRegressor(depth=3, l2_leaf_reg=5)\n",
    "    #cb = cab.CatBoostRegressor(depth=2, l2_leaf_reg=3, learning_rate=0.1, iterations=100, loss_function='RMSE', random_seed=42, verbose=False)\n",
    "    cb = cab.CatBoostRegressor(depth=2, l2_leaf_reg=4, learning_rate=0.15, iterations=100, loss_function='RMSE', random_seed=42, verbose=False)\n",
    "    cb.fit(feature_train, hba1c_train)\n",
    "    t1_results_cb.append(np.nanmean(cb.predict(feature_test)))\n",
    "    t1_e_results_cb.append(np.nanmean(hba1c_test))\n",
    "\n",
    "    # Light GBM\n",
    "    # lg = ltb.LGBMRegressor(boosting_type='dart')\n",
    "    #lg = ltb.LGBMRegressor(\n",
    "    #application='regression',\n",
    "    #objective='root_mean_squared_error',\n",
    "    #boosting_type='gbdt',\n",
    "    #learning_rate=0.05,\n",
    "    #num_leaves=31,\n",
    "    #max_depth=-1,\n",
    "    #min_data_in_leaf=5,\n",
    "    #metric='rmse',\n",
    "    #feature_fraction=0.8,\n",
    "    #bagging_fraction=0.8,\n",
    "    #bagging_freq=5,\n",
    "    #random_state=42,\n",
    "    #n_estimators=500\n",
    "    #)\n",
    "\n",
    "   \n",
    "    lg = ltb.LGBMRegressor(\n",
    "    application='regression',\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=20,\n",
    "    max_depth=2,\n",
    "    min_data_in_leaf=3,\n",
    "    n_estimators=180,\n",
    "    objective='root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    feature_fraction=0.6,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=3,\n",
    "    \n",
    "    )\n",
    " \n",
    "    lg.fit(feature_train, hba1c_train)\n",
    "    t1_results_lg.append(np.nanmean(lg.predict(feature_test)))\n",
    "    t1_e_results_lg.append(np.nanmean(hba1c_test))\n",
    "\n",
    "\n",
    "\n",
    "    t1_test_cases.append(X_test)\n",
    "    t1_train_cases.append(X_train)\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "loo                = sk.model_selection.LeaveOneOut()\n",
    "t1_scores          = []\n",
    "t1_test_cases      = []\n",
    "t1_train_cases     = []\n",
    "countEpoch         = 0\n",
    "percent            = 0.02\n",
    "# XG Boost\n",
    "t1_results_xg      = []\n",
    "t1_e_results_xg    = []\n",
    "# Random Forest\n",
    "t1_results_rf      = []\n",
    "t1_e_results_rf    = []\n",
    "# Cat Boost\n",
    "t1_results_cb      = []\n",
    "t1_e_results_cb    = []\n",
    "# Light GBM\n",
    "t1_results_lg      = []\n",
    "t1_e_results_lg    = []\n",
    "\n",
    "for train_index, test_index in loo.split(PPG_csvArray):\n",
    "    print('Epoch # '+str(countEpoch))\n",
    "    countEpoch +=1\n",
    "\n",
    "    X_train, X_test      = PPG_csvArray[train_index], PPG_csvArray[test_index]\n",
    "    y_train, y_test      = HbA1c_Array [train_index], HbA1c_Array [test_index]\n",
    "    sp_train, sp_test    = SpO2_Array  [train_index], SpO2_Array  [test_index]\n",
    "    bmi_train, bmi_test  = BMI_Array   [train_index], BMI_Array   [test_index]\n",
    "    \n",
    "    \n",
    "    # > __Algorithm 06v1:__ Constructing Features Set _(Red, Green, Blue; Reflected, FeatureCount=[4+4+r1+r2]+[HbA1c]=11)_\n",
    "    print('*************************************************************')\n",
    "    print('*********************Training Array Set**********************')\n",
    "    print('*************************************************************')\n",
    "    train_array = construct_feature_set_v1(\n",
    "        PPG          = X_train,\n",
    "        HbA1c        = y_train,\n",
    "        SpO2         = sp_train,\n",
    "        BMI          = bmi_train,\n",
    "        samplingRate = 24\n",
    "        )\n",
    "    print('*************************************************************')\n",
    "    print('*********************Testing Array Set***********************')\n",
    "    print('*************************************************************')\n",
    "    test_array  = construct_feature_set_v1(\n",
    "        PPG          = X_test,\n",
    "        HbA1c        = y_test,\n",
    "        SpO2         = sp_test,\n",
    "        BMI          = bmi_test,\n",
    "        samplingRate = 24\n",
    "        )\n",
    "    \n",
    "    feature_train, hba1c_train = train_array[:,:-1], train_array[:,-1]\n",
    "    feature_test, hba1c_test   = test_array[:,:-1],  test_array[:,-1]\n",
    "    \n",
    "    train = np.sort(np.array(random.sample(range(feature_test.shape[0]), int(feature_test.shape[0] * percent))))\n",
    "    if train.shape[0] == 0:\n",
    "        feature_train, hba1c_train = np.append(feature_train, np.expand_dims(feature_test[0],axis=0), 0), np.append(hba1c_train, np.expand_dims(hba1c_test[0],axis=0), 0)\n",
    "        # feature_test, hba1c_test = np.delete(feature_test, 0, axis = 0), np.delete(hba1c_test, 0, axis = 0)\n",
    "    else:\n",
    "        test = np.delete(np.arange(feature_test.shape[0]), train)\n",
    "\n",
    "        feature_train, hba1c_train = np.append(feature_train, feature_test[train], 0), np.append(hba1c_train, hba1c_test[train], 0)\n",
    "        feature_test, hba1c_test = np.delete(feature_test, train, axis = 0), np.delete(hba1c_test, train, axis = 0)\n",
    "    \n",
    "    feature_train, hba1c_train = sk.utils.shuffle(feature_train, hba1c_train, random_state=0)\n",
    "    # print(feature_train.shape)\n",
    "    # print(feature_test.shape)\n",
    "\n",
    "\n",
    "    # Random Forest\n",
    "    #rf = RandomForestRegressor(random_state=42, n_estimators=1000, n_jobs=-1)\n",
    "    #rf = RandomForestRegressor(random_state=42, n_estimators=100, min_samples_leaf=1, min_samples_split=5, n_jobs=-1)\n",
    "    #rf = RandomForestRegressor(random_state=42,max_depth=8, n_jobs=-1)\n",
    "    rf = RandomForestRegressor(random_state=42, n_estimators=200, max_depth=3, min_samples_leaf=3, min_samples_split=3, n_jobs=-1)\n",
    "    rf.fit(feature_train, hba1c_train)\n",
    "    t1_results_rf.append(np.nanmean(rf.predict(feature_test)))\n",
    "    t1_e_results_rf.append(np.nanmean(hba1c_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    t1_test_cases.append(X_test)\n",
    "    t1_train_cases.append(X_train)\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39==env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
